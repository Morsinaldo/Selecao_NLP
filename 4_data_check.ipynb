{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4 - data_check.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0I4pgzLVtBTP"
      },
      "source": [
        "# 1.0 Um problema de classificação de ponta-a-ponta usando NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dh34gim6KPtT"
      },
      "source": [
        "## 1.1 Descrição do dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iE8OJoDZ5AFK"
      },
      "source": [
        "O dataset contém informações gerais de 5.000 processos julgados\n",
        "nos Juizados Especiais Federais dentro das Seções Judiciárias do Tribunal Regional Federal da 5a Região (https://www.trf5.jus.br). Os dados são oriundos da raspagem da consulta pública processual. Além disso, ele possui 46 colunas, das quais duas possuem texto livre:\n",
        "\"conteudo_sentenca\" e \"conteudo_acordao\".\n",
        "\n",
        "O dataset pode ser baixado no link a seguir: https://jacob.al/dataset_juizados \n",
        "\n",
        "Ao longo dos notebooks, vão ser realizados os seguintes passos:\n",
        "\n",
        "1. Importação do dataset **(concluído)**\n",
        "2. Análise exploratória dos dados **(concluído)**\n",
        "3. Pré-processamento **(concluído)**\n",
        "4. Verificação dos dados\n",
        "5. Segregação dos dados\n",
        "6. Treinamento\n",
        "7. Teste\n",
        "\n",
        "<center><img width=\"600\" src=\"https://drive.google.com/uc?export=view&id=1fKGuR5U5ECf7On6Zo1UWzAIWZrMmZnGc\"></center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UpxKxU1Ej7f"
      },
      "source": [
        "## 1.2 Instalação e importação das bibliotecas e configuração do wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t82KewAPWCYe"
      },
      "outputs": [],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytest pytest-sugar"
      ],
      "metadata": {
        "id": "IumW4s8Sh9i_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LASaVZuhRJlL"
      },
      "outputs": [],
      "source": [
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Login no Weights & Biases\n",
        "!wandb login --relogin"
      ],
      "metadata": {
        "id": "QZXcN54GkP25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87a3c739-bd35-46ad-ebad-9ebbc2465099"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Pytest\n"
      ],
      "metadata": {
        "id": "MPjpyeU7d37l"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJjWla1qxd3i"
      },
      "source": [
        "### 1.2.1 Criação e execução do arquivo de teste\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9acpZigRVANF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d00b0409-bb66-44ae-f475-21d4eb40339c"
      },
      "source": [
        "%%file test_data.py\n",
        "import pytest\n",
        "import wandb\n",
        "import pandas as pd\n",
        "\n",
        "# This is global so all tests are collected under the same run\n",
        "run = wandb.init(project=\"nlp_bolsa\", job_type=\"data_checks\")\n",
        "\n",
        "@pytest.fixture(scope=\"session\")\n",
        "def data():\n",
        "\n",
        "    local_path = run.use_artifact(\"nlp_bolsa/preprocessed_data.csv:latest\").file()\n",
        "    df = pd.read_csv(local_path)\n",
        "\n",
        "    return df\n",
        "\n",
        "def test_data_length(data):\n",
        "    \"\"\"\n",
        "    Nosso teste possui dados suficientes para continuar\n",
        "    \"\"\"\n",
        "    assert len(data) > 1000\n",
        "\n",
        "\n",
        "def test_number_of_columns(data):\n",
        "    \"\"\"\n",
        "    Nós teste possui o número de colunas para continuar\n",
        "    \"\"\"\n",
        "    assert data.shape[1] == 2\n",
        "\n",
        "def test_column_presence_and_type(data):\n",
        "\n",
        "    required_columns = {\n",
        "        \"assunto_cnj\": pd.api.types.is_object_dtype,\n",
        "        \"conteudo_sentenca\": pd.api.types.is_object_dtype\n",
        "    }\n",
        "\n",
        "    # Verifica a presença da coluna\n",
        "    assert set(data.columns.values).issuperset(set(required_columns.keys()))\n",
        "\n",
        "    for col_name, format_verification_funct in required_columns.items():\n",
        "\n",
        "        assert format_verification_funct(data[col_name]), f\"Column {col_name} failed test {format_verification_funct}\"\n",
        "\n",
        "\n",
        "def test_class_names(data):\n",
        "\n",
        "    # Check that only the known classes are present\n",
        "    known_classes = [\n",
        "        \"Direito Tributário\",\n",
        "        \"Direito Civil\",\n",
        "        \"Direito Previdenciário\",\n",
        "        \"Direito Administrativo e outras matérias do Direito Público\",\n",
        "        \"Direito do Consumidor\",\n",
        "        \"Direito Processual Civil e do Trabalho\",\n",
        "        \"Direito do Trabalho\"\n",
        "    ]\n",
        "\n",
        "    assert data[\"assunto_cnj\"].isin(known_classes).all()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test_data.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXBQkMc8VeD8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "429bbab3-37c0-4446-d3d0-dbba0aa2a69e"
      },
      "source": [
        "!pytest . -vv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mTest session starts (platform: linux, Python 3.7.13, pytest 3.6.4, pytest-sugar 0.9.5)\u001b[0m\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile:\n",
            "plugins: typeguard-2.7.1, sugar-0.9.5\n",
            "\n",
            " \u001b[36mtest_data.py\u001b[0m::test_data_length\u001b[0m \u001b[32m✓\u001b[0m                                 \u001b[32m25% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█▌       \u001b[0m\n",
            " \u001b[36mtest_data.py\u001b[0m::test_number_of_columns\u001b[0m \u001b[32m✓\u001b[0m                           \u001b[32m50% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m██     \u001b[0m\n",
            " \u001b[36mtest_data.py\u001b[0m::test_column_presence_and_type\u001b[0m \u001b[32m✓\u001b[0m                    \u001b[32m75% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m██\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█▌  \u001b[0m\n",
            " \u001b[36mtest_data.py\u001b[0m::test_class_names\u001b[0m \u001b[32m✓\u001b[0m                                \u001b[32m100% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m██\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m██\u001b[0m\n",
            "\n",
            "Results (6.18s):\n",
            "\u001b[32m       4 passed\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encerrando o run\n",
        "run.finish()"
      ],
      "metadata": {
        "id": "5284u1A7euMF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}